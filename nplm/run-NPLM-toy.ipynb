{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f063793-dbcf-446c-b0c7-08855a17c1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, h5py, math, time, os, json, argparse, datetime\n",
    "import numpy as np\n",
    "from FLKutils import *\n",
    "from SampleUtils import *\n",
    "\n",
    "import matplotlib as mpl\n",
    "#mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.style.use('classic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d64d63-3567-44a4-a253-e626217eee31",
   "metadata": {},
   "source": [
    "# Problem setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc228341-ec5d-41fc-88f0-923635976ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels identifying the signal classes in the dataset\n",
    "sig_labels=[3]\n",
    "# labels identifying the background classes in the dataset\n",
    "bkg_labels=[0, 1, 2]\n",
    "\n",
    "# hyper parameters of the NPLM model based on kernel methods\n",
    "## number of kernels\n",
    "M = 1000 \n",
    "\n",
    "## percentile of the distribution of pair-wise distances between reference-distributed points\n",
    "flk_sigma_perc=90 \n",
    "\n",
    "## L2 regularization coefficient\n",
    "lam =1e-6 \n",
    "\n",
    "## number of maximum iterations before the training is killed\n",
    "iterations=1000000 \n",
    "\n",
    "## number of toys to simulate \n",
    "## (multiple experiments allow you to build a statistics for the test and quantify type I and II errors)\n",
    "Ntoys = 10 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd32423-b17c-48e1-a0cf-44c502c9d42d",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca90aefb-40d9-4179-85e6-e9cc7e230866",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load('./predictions/simclr_predictions.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c30b9a2-1b92-4aed-8200-7ae0183b5933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['instances',\n",
       " 'dim2',\n",
       " 'dim4',\n",
       " 'dim8',\n",
       " 'dim16',\n",
       " 'dim24',\n",
       " 'dim8_embedding',\n",
       " 'dim2_embedding',\n",
       " 'dim16_embedding',\n",
       " 'dim24_embedding',\n",
       " 'dim32_embedding']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f290c0f6-9774-485e-b3b3-1b9309f71535",
   "metadata": {},
   "outputs": [],
   "source": [
    "target=a['dim8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aa6b1f33-6f43-4014-bad7-a61673cf6a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ff4e1ac-db02-453c-8844-8389a26ab589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 2048, 4)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['dim8_embedding'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4be34c7c-0963-40eb-b002-7f0f550b10ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=a['dim8_embedding'].reshape((-1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb6328bf-1294-402e-8e93-4ecc0a2267d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40960, 4)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "45883964-6f80-4c8b-8f79-28bfc349e630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40960,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b0c118-cab1-4529-8b20-75793654cb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ begin load data\n",
    "# This part needs to be modified according to how the predictions of your model are stored.\n",
    "# Here the predictions are saved in npz files\n",
    "print('Load data')\n",
    "file = './predictions/simclr_predictions.npz'\n",
    "f = np.load(file)\n",
    "features = f['dim8_embedding'].reshape((-1, 4))\n",
    "target = #...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0a8c4041-719e-457f-9de5-0c01d82e6af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select SIG and BKG classes\n",
    "mask_SIG = np.zeros_like(target)\n",
    "mask_BKG = np.zeros_like(target)\n",
    "for sig_label in sig_labels:\n",
    "    mask_SIG += 1*(target==sig_label)\n",
    "for bkg_label in bkg_labels:\n",
    "    mask_BKG += 1*(target==bkg_label)\n",
    "\n",
    "features_SIG = features[mask_SIG>0]\n",
    "features_BKG = features[mask_BKG>0]\n",
    "############ end load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b66d87dd-1450-4601-9edb-83ecbc640d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardize\n",
      "mean:  [-2.13186052 -2.12510567 -2.50298683 -3.32404615]\n",
      "std:  [0.96278889 0.76894639 0.7268368  0.35038829]\n"
     ]
    }
   ],
   "source": [
    "######## standardizes data\n",
    "print('standardize')\n",
    "features_mean, features_std = np.mean(features_BKG, axis=0), np.std(features_BKG, axis=0)\n",
    "print('mean: ', features_mean)\n",
    "print('std: ', features_std)\n",
    "features_BKG = standardize(features_BKG, features_mean, features_std)\n",
    "features_SIG = standardize(features_SIG, features_mean, features_std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5bea0580-ef4f-4a3d-97e7-2eed68a5b7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flk_sigma 4.0\n"
     ]
    }
   ],
   "source": [
    "#### compute sigma hyper parameter from data\n",
    "#### sigma is the gaussian kernels width. \n",
    "#### Following a heuristic, we set this hyperparameter to the 90% quantile of the distribution of pair-wise distances between bkg-distributed points\n",
    "#### (see below)\n",
    "#### This doesn't need modifications, but one could in principle change it (see https://arxiv.org/abs/2408.12296)\n",
    "flk_sigma = candidate_sigma(features_BKG[:2000, :], perc=flk_sigma_perc)\n",
    "print('flk_sigma', flk_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3df3afe-437c-48d0-a989-ccec8bfd40b0",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aca8082-1e2d-4334-bb10-d543cc95822e",
   "metadata": {},
   "source": [
    "## null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2e588807-e804-4c6f-9bba-a2cc2dcef75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ref = 10000 # number of reference datapoints (mixture of non-anomalous classes)\n",
    "N_bkg = 1000 # number of backgroun datapoints in the data (mixture of non-anomalous classes present in the data)\n",
    "N_sig = 0 # number of signal datapoints in the data (mixture of anomalous classes present in the data)\n",
    "w_ref = N_bkg*1./N_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f81ae0-c650-4b07-9470-6bf79bf17c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run toys\n",
    "print('Start running toys')\n",
    "t0=np.array([])\n",
    "seeds = np.random.randint(low=1, high=100000, size=(Ntoys,))\n",
    "for i in range(Ntoys):\n",
    "    seed = seeds[i]\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    N_bkg_p = rng.poisson(lam=N_bkg, size=1)[0]\n",
    "    N_sig_p = rng.poisson(lam=N_sig, size=1)[0]\n",
    "    rng.shuffle(features_SIG)\n",
    "    rng.shuffle(features_BKG)\n",
    "    features_s = features_SIG[:N_sig_p, :]\n",
    "    features_b = features_BKG[:N_bkg_p+N_ref, :]\n",
    "    features  = np.concatenate((features_s,features_b), axis=0)\n",
    "\n",
    "    label_R = np.zeros((N_ref,))\n",
    "    label_D = np.ones((N_bkg_p+N_sig_p, ))\n",
    "    labels  = np.concatenate((label_D,label_R), axis=0)\n",
    "    \n",
    "    flk_config = get_logflk_config(M,flk_sigma,[lam],weight=w_ref,iter=[iterations],seed=None,cpu=False)\n",
    "    t, pred = run_toy('t0', features, labels, weight=w_ref, seed=seed,\n",
    "                      flk_config=flk_config, output_path='./', plot=False, savefig=False,\n",
    "                      verbose=verbose)\n",
    "    \n",
    "    t0 = np.append(t0, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8374ae78-ebd4-44ae-97dc-e53733e1a2b5",
   "metadata": {},
   "source": [
    "## alternative hypothesis (signal injection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18e2700-0c02-451f-9a41-d946d169dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ref = 10000 # number of reference datapoints (mixture of non-anomalous classes)\n",
    "N_bkg = 1000 # number of backgroun datapoints in the data (mixture of non-anomalous classes present in the data)\n",
    "N_sig = 100 # number of signal datapoints in the data (mixture of anomalous classes present in the data)\n",
    "w_ref = N_bkg*1./N_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c70d61-b30a-4adf-b98f-9dbdf8c83dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run toys\n",
    "print('Start running toys')\n",
    "t1=np.array([])\n",
    "seeds = np.random.randint(low=1, high=100000, size=(Ntoys,))\n",
    "for i in range(Ntoys):\n",
    "    seed = seeds[i]\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    N_bkg_p = rng.poisson(lam=N_bkg, size=1)[0]\n",
    "    N_sig_p = rng.poisson(lam=N_sig, size=1)[0]\n",
    "    rng.shuffle(features_SIG)\n",
    "    rng.shuffle(features_BKG)\n",
    "    features_s = features_SIG[:N_sig_p, :]\n",
    "    features_b = features_BKG[:N_bkg_p+N_ref, :]\n",
    "    features  = np.concatenate((features_s,features_b), axis=0)\n",
    "\n",
    "    label_R = np.zeros((N_ref,))\n",
    "    label_D = np.ones((N_bkg_p+N_sig_p, ))\n",
    "    labels  = np.concatenate((label_D,label_R), axis=0)\n",
    "    \n",
    "    flk_config = get_logflk_config(M,flk_sigma,[lam],weight=w_ref,iter=[iterations],seed=None,cpu=False)\n",
    "    t, pred = run_toy(manifold, features, labels, weight=w_ref, seed=seed,\n",
    "                      flk_config=flk_config, output_path='./', plot=False, savefig=False,\n",
    "                      verbose=verbose)\n",
    "    \n",
    "    t1 = np.append(t1, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3a8787-34c4-468a-8ea3-6e619557ca66",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a882629b-455f-441e-bce1-db67bf498f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Z_score_chi2(t,df):\n",
    "    sf = chi2.sf(t, df)\n",
    "    Z  = -norm.ppf(sf)\n",
    "    return Z\n",
    "\n",
    "def Z_score_norm(t,mu, std):\n",
    "    sf = norm.sf(t, mu, std)\n",
    "    Z  = -norm.ppf(sf)\n",
    "    return Z\n",
    "\n",
    "def plot_1distribution(t, df, xmin=0, xmax=300, nbins=10, save=False, ymax=None, output_path='', save_name='', label=''):\n",
    "    '''\n",
    "    Plot the histogram of a test statistics sample (t) and the target chi2 distribution (df must be specified!). \n",
    "    The median and the error on the median are calculated in order to calculate the median Z-score and its error.\n",
    "    '''\n",
    "    plt.rcParams[\"font.family\"] = \"serif\"\n",
    "    plt.style.use('classic')\n",
    "    fig  = plt.figure(figsize=(12, 9))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    # plot distribution histogram\n",
    "    bins      = np.linspace(xmin, xmax, nbins+1)\n",
    "    Z_obs     = norm.ppf(chi2.cdf(np.median(t), df))\n",
    "    t_obs_err = 1.2533*np.std(t)*1./np.sqrt(t.shape[0])\n",
    "    Z_obs_p   = norm.ppf(chi2.cdf(np.median(t)+t_obs_err, df))\n",
    "    Z_obs_m   = norm.ppf(chi2.cdf(np.median(t)-t_obs_err, df))\n",
    "    label  = 'sample %s\\nsize: %i \\nmedian: %s, std: %s\\n'%(label, t.shape[0], str(np.around(np.median(t), 2)),str(np.around(np.std(t), 2)))\n",
    "    label += 'Z = %s (+%s/-%s)'%(str(np.around(Z_obs, 2)), str(np.around(Z_obs_p-Z_obs, 2)), str(np.around(Z_obs-Z_obs_m, 2)))\n",
    "    binswidth = (xmax-xmin)*1./nbins\n",
    "    h = plt.hist(t, weights=np.ones_like(t)*1./(t.shape[0]*binswidth), color='lightblue', ec='#2c7fb8',\n",
    "                 bins=bins, label=label)\n",
    "    err = np.sqrt(h[0]/(t.shape[0]*binswidth))\n",
    "    x   = 0.5*(bins[1:]+bins[:-1])\n",
    "    plt.errorbar(x, h[0], yerr = err, color='#2c7fb8', marker='o', ls='')\n",
    "    # plot reference chi2\n",
    "    x  = np.linspace(chi2.ppf(0.0001, df), chi2.ppf(0.9999, df), 100)\n",
    "    plt.plot(x, chi2.pdf(x, df),'midnightblue', lw=5, alpha=0.8, label=r'$\\chi^2_{%i}$'%(df))\n",
    "    font = font_manager.FontProperties(family='serif', size=14) \n",
    "    plt.legend(prop=font, frameon=False)\n",
    "    plt.xlabel('t', fontsize=18, fontname=\"serif\")\n",
    "    plt.ylabel('Probability', fontsize=18, fontname=\"serif\")\n",
    "    plt.yticks(fontsize=16, fontname=\"serif\")\n",
    "    plt.xticks(fontsize=16, fontname=\"serif\")\n",
    "    if ymax !=None:\n",
    "        plt.ylim(0., ymax)\n",
    "    if save:\n",
    "        if output_path=='':\n",
    "            print('argument output_path is not defined. The figure will not be saved.')\n",
    "        else:\n",
    "            plt.savefig(output_path+ save_name+'_distribution.pdf')\n",
    "            print('saved at %s'%(output_path+ save_name+'_distribution.pdf'))\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    return\n",
    "\n",
    "def plot_2distribution(t1, t2, df, xmin=0, xmax=300, ymax=None, nbins=10, save=False, output_path='', label1='1', label2='2', save_name='', print_Zscore=True):\n",
    "    '''\n",
    "    Plot the histogram of a test statistics sample (t) and the target chi2 distribution (df must be specified!).\n",
    "    The median and the error on the median are calculated in order to calculate the median Z-score and its error.\n",
    "    '''\n",
    "    plt.rcParams[\"font.family\"] = \"serif\"\n",
    "    plt.style.use('classic')\n",
    "    fig  = plt.figure(figsize=(12, 9))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    # plot distribution histogram\n",
    "    bins      = np.linspace(xmin, xmax, nbins+1)\n",
    "    binswidth = (xmax-xmin)*1./nbins\n",
    "    # t1\n",
    "    Z_obs     = Z_score_chi2(np.median(t1), df)\n",
    "    t_obs_err = 1.2533*np.std(t1)*1./np.sqrt(t1.shape[0])\n",
    "    Z_obs_p   = Z_score_chi2(np.median(t1)+t_obs_err, df)\n",
    "    Z_obs_m   = Z_score_chi2(np.median(t1)-t_obs_err, df)\n",
    "    label  = '%s \\nsize: %i\\nmedian: %s, std: %s\\n'%(label1, t1.shape[0], str(np.around(np.median(t1), 2)),str(np.around(np.std(t1), 2)))\n",
    "    if print_Zscore:\n",
    "        label += 'asymptotic Z = %s (+%s/-%s)'%(str(np.around(Z_obs, 2)), str(np.around(Z_obs_p-Z_obs, 2)), str(np.around(Z_obs-Z_obs_m, 2)))\n",
    "    \n",
    "    h = plt.hist(t1, weights=np.ones_like(t1)*1./(t1.shape[0]*binswidth), color='lightblue', ec='#2c7fb8',\n",
    "                 bins=bins, label=label)\n",
    "    err = np.sqrt(h[0]/(t1.shape[0]*binswidth))\n",
    "    x   = 0.5*(bins[1:]+bins[:-1])\n",
    "    plt.errorbar(x, h[0], yerr = err, color='#2c7fb8', marker='o', ls='')\n",
    "    max1 = np.max(h[0])\n",
    "    # t2\n",
    "    Z_obs     = Z_score_chi2(np.median(t2), df)\n",
    "    t_obs_err = 1.2533*np.std(t2)*1./np.sqrt(t2.shape[0])\n",
    "    Z_obs_p   = Z_score_chi2(np.median(t2)+t_obs_err, df)\n",
    "    Z_obs_m   = Z_score_chi2(np.median(t2)-t_obs_err, df)\n",
    "    t_empirical = np.sum(1.*(t1>np.mean(t2)))*1./t1.shape[0]\n",
    "    empirical_lim = '='\n",
    "    if t_empirical==0:\n",
    "        empirical_lim='>'\n",
    "        t_empirical = 1./t1.shape[0]\n",
    "    t_empirical_err = t_empirical*np.sqrt(1./np.sum(1.*(t1>np.mean(t2))+1./t1.shape[0]))\n",
    "    Z_empirical = norm.ppf(1-t_empirical)\n",
    "    Z_empirical_m = norm.ppf(1-(t_empirical+t_empirical_err))\n",
    "    Z_empirical_p = norm.ppf(1-(t_empirical-t_empirical_err))\n",
    "                                          \n",
    "    label  = '%s \\nsize: %i\\nmedian: %s, std: %s\\n'%(label2, t2.shape[0], str(np.around(np.median(t2), 2)),str(np.around(np.std(t2), 2)))\n",
    "    if print_Zscore:\n",
    "        label += 'asymptotic Z = %s (+%s/-%s) \\n'%(str(np.around(Z_obs, 2)), str(np.around(Z_obs_p-Z_obs, 2)), str(np.around(Z_obs-Z_obs_m, 2)))\n",
    "        label += 'empirical Z %s %s (+%s/-%s)'%(empirical_lim, str(np.around(Z_empirical, 2)), str(np.around(Z_empirical_p-Z_empirical, 2)), str(np.around(Z_empirical-Z_empirical_m, 2)))\n",
    "    h = plt.hist(t2, weights=np.ones_like(t2)*1./(t2.shape[0]*binswidth), color='#8dd3c7', ec='seagreen',\n",
    "                 bins=bins, label=label)\n",
    "    err = np.sqrt(h[0]/(t2.shape[0]*binswidth))\n",
    "    x   = 0.5*(bins[1:]+bins[:-1])\n",
    "    plt.errorbar(x, h[0], yerr = err, color='seagreen', marker='o', ls='')\n",
    "    max2 = np.max(h[0])\n",
    "    # plot reference chi2\n",
    "    x  = np.linspace(chi2.ppf(0.0001, df), chi2.ppf(0.9999, df), 100)\n",
    "    plt.plot(x, chi2.pdf(x, df),'midnightblue', lw=5, alpha=0.8, label=r'$\\chi^2_{%i}$'%(df))\n",
    "    font = font_manager.FontProperties(family='serif', size=20) #weight='bold', style='normal', \n",
    "    plt.legend(ncol=1, loc='upper right', prop=font, frameon=False)\n",
    "    plt.xlabel('$t$', fontsize=32, fontname=\"serif\")\n",
    "    plt.ylabel('Probability', fontsize=32, fontname=\"serif\")\n",
    "    plt.ylim(0., 1.2*np.maximum(max1, max2))#np.max(chi2.pdf(x, df))*1.3)\n",
    "    if ymax !=None:\n",
    "        plt.ylim(0., ymax)\n",
    "    plt.yticks(fontsize=22, fontname=\"serif\")\n",
    "    plt.xticks(fontsize=22, fontname=\"serif\")\n",
    "    if save:\n",
    "        if output_path=='':\n",
    "            print('argument output_path is not defined. The figure will not be saved.')\n",
    "        else:\n",
    "            plt.savefig(output_path+ save_name+'_2distribution.pdf')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return #[Z_obs, Z_obs_p, Z_obs_m], [Z_empirical, Z_empirical_p, Z_empirical_m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9be9ac-f95d-4f6d-9b9b-88a42551e188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot null distribution\n",
    "plot_1distribution(t0, df=np.mean(t0), xmin=0.9*np.min(t0), xmax=60, nbins=16, label='REF', save=False, ymax=None, output_path='', save_name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6980b0-9706-4647-842c-4cc4ebcca266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot alternative vs null distributions\n",
    "plot_2distribution(t0, t1, df=np.mean(t0), xmin=np.min(t0), xmax=np.max(t1)*1.1, #ymax=0.03, \n",
    "                   nbins=19, label1='REF', label2='BKG+SIG', print_Zscore=True,\n",
    "                   save=False, output_path='', save_name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ae7877-b529-4029-a232-d368b4d11d66",
   "metadata": {},
   "source": [
    "# save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c157bcd-5237-4825-b05f-b62ff5d8a21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## details about the save path\n",
    "folder_out = './out/'\n",
    "sig_string = ''\n",
    "if N_sig:\n",
    "    sig_string+='_SIG'\n",
    "    for s in sig_labels:\n",
    "        sig_string+='-%i'%(s)\n",
    "NP = '%s_NR%i_NB%i_NS%i_M%i_lam%s_iter%i/'%(sig_string, N_ref, N_bkg, N_sig,\n",
    "                                                  M, str(lam), iterations)\n",
    "if not os.path.exists(folder_out+NP):\n",
    "    os.makedirs(folder_out+NP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f018e05-59b7-43f9-8a91-d20965b37638",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('%s/t0.npy'%(folder_out+NP), t0)\n",
    "np.save('%s/t1.npy'%(folder_out+NP), t1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
